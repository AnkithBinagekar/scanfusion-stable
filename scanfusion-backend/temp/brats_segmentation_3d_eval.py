# -*- coding: utf-8 -*-
"""brats_segmentation_3d_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JcbkcR1b8WrHcWsrqs8bK4rtomn41x-3

Copyright (c) MONAI Consortium  
Licensed under the Apache License, Version 2.0 (the "License");  
you may not use this file except in compliance with the License.  
You may obtain a copy of the License at  
&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  
Unless required by applicable law or agreed to in writing, software  
distributed under the License is distributed on an "AS IS" BASIS,  
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  
See the License for the specific language governing permissions and  
limitations under the License.

# Brain tumor 3D segmentation with MONAI

This tutorial shows how to construct a training workflow of multi-labels segmentation task.

And it contains below features:
1. Transforms for dictionary format data.
1. Define a new transform according to MONAI transform API.
1. Load Nifti image with metadata, load a list of images and stack them.
1. Randomly adjust intensity for data augmentation.
1. Cache IO and transforms to accelerate training and validation.
1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.
1. Deterministic training for reproducibility.

The dataset comes from http://medicaldecathlon.com/.  
Target: Gliomas segmentation necrotic/active tumour and oedema  
Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  
Size: 750 4D volumes (484 Training + 266 Testing)  
Source: BRATS 2016 and 2017 datasets.  
Challenge: Complex and heterogeneously-located targets

Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).
(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))

![image](https://github.com/Project-MONAI/tutorials/blob/main/figures/brats_tasks.png?raw=1)

The image patches show from left to right:
1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).
1. the tumor core (red) visible in T2 (Fig.B).
1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).
1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)

## Setup environment
"""

# Commented out IPython magic to ensure Python compatibility.
#!python -c "import monai" || pip install -q "monai-weekly[nibabel, tqdm]"
#!python -c "import matplotlib" || pip install -q matplotlib
# !python -c "import onnxruntime" || pip install -q onnxruntime
# %matplotlib inline

"""## Setup imports"""

import os
import shutil
import tempfile
import time
import matplotlib.pyplot as plt
from monai.apps import DecathlonDataset
from monai.config import print_config
from monai.data import DataLoader, decollate_batch
from monai.handlers.utils import from_engine
from monai.losses import DiceLoss
from monai.inferers import sliding_window_inference
from monai.metrics import DiceMetric
from monai.networks.nets import SegResNet
from monai.transforms import (
    Activations,
    Activationsd,
    AsDiscrete,
    AsDiscreted,
    Compose,
    Invertd,
    LoadImaged,
    MapTransform,
    NormalizeIntensityd,
    Orientationd,
    RandFlipd,
    RandScaleIntensityd,
    RandShiftIntensityd,
    RandSpatialCropd,
    Spacingd,
    EnsureTyped,
    EnsureChannelFirstd,
)
from monai.utils import set_determinism
# import onnxruntime
from tqdm import tqdm

import torch

print_config()

"""## Setup transforms for training and validation"""

val_transform = Compose(
    [
        LoadImaged(keys=["image"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image"]),
        # ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image"], axcodes="RAS"),
        Spacingd(
            keys=["image"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear"),
        ),
        NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
    ]
)

"""## Quickly load data with DecathlonDataset

Here we use `DecathlonDataset` to automatically download and extract the dataset.
It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size.
"""

from monai.data import Dataset, DataLoader

# ====== CONFIG ======
is_onlyFlair = True  # Set this flag to switch between full input or FLAIR-only

# ====== PATHS ======
flair_path = "UCSF-PDGM-0007_FLAIR.nii.gz"
t1_path    = "UCSF-PDGM-0004_T1.nii.gz"
t1ce_path  = "UCSF-PDGM-0004_T1c.nii.gz"
t2_path    = "UCSF-PDGM-0004_T2.nii.gz"

# ====== DATASET DEFINITION ======
if is_onlyFlair:
    my_data = [{"image": flair_path}]
else:
    my_data = [{"image": [flair_path, t1_path, t1ce_path, t2_path]}]

# ====== MONAI Dataset ======
val_ds = Dataset(data=my_data, transform=val_transform)
val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)

"""## Check best pytorch model output with the input image and label"""

VAL_AMP = True

device = torch.device("cpu")
model = SegResNet(
    blocks_down=[1, 2, 2, 4],
    blocks_up=[1, 1, 1],
    init_filters=16,
    in_channels=4,
    out_channels=3,
    dropout_prob=0.2,
).to(device)


dice_metric = DiceMetric(include_background=True, reduction="mean")
dice_metric_batch = DiceMetric(include_background=True, reduction="mean_batch")

post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])


# define inference method
def inference(input):
    def _compute(input):
        return sliding_window_inference(
            inputs=input,
            roi_size=(240, 240, 160),
            sw_batch_size=1,
            predictor=model,
            overlap=0.5,
        )

    if VAL_AMP:
        with torch.autocast("cpu"):
            return _compute(input)
    else:
        return _compute(input)


# use amp to accelerate training
scaler = torch.GradScaler("cpu")
# enable cuDNN benchmark
torch.backends.cudnn.benchmark = True

"""## All Slices Output"""

 

import os
import matplotlib.pyplot as plt
import  cv2
import numpy as np

# ==== CONFIG ====
# is_onlyFlair = True  # Set True for only FLAIR input

# ==== OUTPUT FOLDER ====
output_dir = "combined_slices_single_date" if is_onlyFlair else "combined_slices"
os.makedirs(output_dir, exist_ok=True)

# ==== LOAD MODEL ====
model.load_state_dict(torch.load("model.pt"))
model.eval()

with torch.no_grad():
    # ==== CONDITIONAL INPUT ====
    if is_onlyFlair:
        # Load only FLAIR and repeat 4 times for 4-channel input
        flair = val_ds[0]["image"][0]  # Only first modality
        input_tensor = flair.unsqueeze(0).repeat(4, 1, 1, 1)  # [4, H, W, D]
    else:
        # Load all 4 modalities (assumed already aligned)
        input_tensor = val_ds[0]["image"]  # [4, H, W, D]

    val_input = input_tensor.unsqueeze(0).to(device)  # [1, 4, H, W, D]

    # ==== INFERENCE ====
    val_output = inference(val_input)
    val_output = post_trans(val_output[0])  # [3, H, W, D]

    num_slices = val_input.shape[-1]

    # ==== SAVE COMBINED SLICE IMAGES ====
    for slice_index in range(num_slices):
        slice_folder = os.path.join(output_dir, f"slice_{slice_index:03d}")
        os.makedirs(slice_folder, exist_ok=True)

        for i in range(1):  # Input channels (e.g., just channel 0)
            plt.imshow(val_input[0][i, :, :, slice_index].detach().cpu(),cmap="gray")
            plt.axis("off")
            plt.savefig(os.path.join(slice_folder, f"input_channel_{i}.jpg"), bbox_inches="tight", pad_inches=0)
            plt.close()



        for i in range(3):  # Output channels
            seg_img = val_output[i, :, :, slice_index].cpu().numpy()
            seg_img = (seg_img * 255).astype(np.uint8)
            cv2.imwrite(os.path.join(slice_folder, f"output_channel_{i}.jpg"), seg_img)

"""## Single Slice Output"""

# Load the pretrained model
model.load_state_dict(torch.load("model.pt"))
model.eval()

with torch.no_grad():
    # Prepare input from single-channel FLAIR by repeating 4 times
    flair = val_ds[0]["image"]  # [1, H, W, D]
    flair_4ch = flair.repeat(4, 1, 1, 1)  # [4, H, W, D]
    val_input = flair_4ch.unsqueeze(0).to(device)  # [1, 4, H, W, D]
    # val_input = val_ds[0]["image"].unsqueeze(0).to(device)

    # Run inference
    val_output = inference(val_input)
    val_output = post_trans(val_output[0])

    slice_index= 130
    # Visualize 4-channel input
    plt.figure("image", (24, 6))
    for i in range(4):
        plt.subplot(1, 4, i + 1)
        plt.title(f"image channel {i}")
        img = val_input[0][i, :, :, slice_index].detach().cpu().numpy()
        plt.imshow(img, cmap="gray")
        # plt.savefig(f"input_{slice_index}")
    plt.show()
    plt.imsave(f'input_slice_{slice_index}.png', img, cmap='gray')

    # Visualize 3-channel model output
    plt.figure("output", (18, 6))
    for i in range(3):
        plt.subplot(1, 3, i + 1)
        plt.title(f"output channel {i}")
        plt.imshow(val_output[i, :, :, slice_index].detach().cpu())
        plt.axis("off")
        plt.savefig(f"output_slice_{slice_index}")
    plt.show()
    # plt.imsave(f'output_slice_{slice_index}.png', val_output[i, :, :, slice_index].detach().cpu())

"""## Ground Truth

"""

import nibabel as nib
from monai.transforms import (
    Compose, LoadImage, EnsureChannelFirst, EnsureType,
    Orientation, Spacing, NormalizeIntensity
)


nifti_path = flair_path
sample = {"image": nifti_path}


mask_transform = Compose(
    [
        LoadImaged(keys=["image"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image"]),
        # ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image"], axcodes="RAS"),
        Spacingd(
            keys=["image"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear"),
        ),
        # NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
    ]
)


transformed = mask_transform(sample)
image_tensor = transformed["image"]
image_np = image_tensor[0].cpu().numpy()


print(image_np.shape)
plt.imshow(image_np[:, :, slice_index], cmap='gray')
plt.title(f"Ground_Truth_{slice_index}")
plt.axis('off')
plt.show()

